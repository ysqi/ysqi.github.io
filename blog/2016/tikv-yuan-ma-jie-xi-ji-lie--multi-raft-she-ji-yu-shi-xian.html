<!DOCTYPE html><html lang="zh"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><title>TiKV源码解析系列-multi-raft设计与实现 - 虞双齐爱折腾</title><link rel="alternate" hreflang="zh" href="https://yushuangqi.com"><meta name="renderer" content="webkit"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="MobileOptimized" content="width"><meta name="HandheldFriendly" content="true"><meta name="applicable-device" content="pc,mobile"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="theme-color" content="#f8f5ec"><meta name="msapplication-navbutton-color" content="#f8f5ec"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec"><meta name="mobile-web-app-capable" content="yes"><meta name="author" content="虞双齐"><meta name="description" content="本系列文章主要面向 TiKV 社区开发者，重点介绍 TiKV 的系统架构，源码结构，流程解析。目的是使得开发者阅读之后，能对 TiKV 项目有一个初步了解，更好的参与进入"><meta name="keywords" content="智能合约开发, Go语言, 区块链技术"><meta name="generator" content="Hugo 0.37.1"><link rel="canonical" href="https://yushuangqi.com/blog/2016/tikv-yuan-ma-jie-xi-ji-lie--multi-raft-she-ji-yu-shi-xian.html"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="icon" href="/favicon.ico"><link rel="manifest" href="/manifest.json"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link href="/dist/jane.min.css?v=2.7.0" rel="stylesheet"><link href="https://cdn.bootcss.com/fancybox/3.1.20/jquery.fancybox.min.css" rel="stylesheet" crossorigin="anonymous"><meta property="og:title" content="TiKV源码解析系列-multi-raft设计与实现"><meta property="og:description" content="本系列文章主要面向 TiKV 社区开发者，重点介绍 TiKV 的系统架构，源码结构，流程解析。目的是使得开发者阅读之后，能对 TiKV 项目有一个初步了解，更好的参与进入"><meta property="og:type" content="article"><meta property="og:url" content="https://yushuangqi.com/blog/2016/tikv-yuan-ma-jie-xi-ji-lie--multi-raft-she-ji-yu-shi-xian.html"><meta property="article:published_time" content="2016-12-31T11:32:35&#43;08:00"><meta property="article:modified_time" content="2016-12-31T11:32:35&#43;08:00"><meta itemprop="name" content="TiKV源码解析系列-multi-raft设计与实现"><meta itemprop="description" content="本系列文章主要面向 TiKV 社区开发者，重点介绍 TiKV 的系统架构，源码结构，流程解析。目的是使得开发者阅读之后，能对 TiKV 项目有一个初步了解，更好的参与进入"><meta itemprop="datePublished" content="2016-12-31T11:32:35&#43;08:00"><meta itemprop="dateModified" content="2016-12-31T11:32:35&#43;08:00"><meta itemprop="wordCount" content="5496"><meta itemprop="keywords" content="rust,golang,mysql,"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="TiKV源码解析系列-multi-raft设计与实现"><meta name="twitter:description" content="本系列文章主要面向 TiKV 社区开发者，重点介绍 TiKV 的系统架构，源码结构，流程解析。目的是使得开发者阅读之后，能对 TiKV 项目有一个初步了解，更好的参与进入"><!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]--><!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]--></head><body><div id="mobile-navbar" class="mobile-navbar"><div class="mobile-header-logo"><a href="/" class="logo">虞双齐爱折腾</a></div><div class="mobile-navbar-icon"><span></span> <span></span> <span></span></div></div><nav id="mobile-menu" class="mobile-menu slideout-menu"><ul class="mobile-menu-list"><a href="/"><li class="mobile-menu-item">首页</li></a><a href="/post.html"><li class="mobile-menu-item">归档</li></a><a href="/tags.html"><li class="mobile-menu-item">标签</li></a><a href="/categories.html"><li class="mobile-menu-item">分类</li></a><a href="/about.html"><li class="mobile-menu-item">关于</li></a></ul></nav><header id="header" class="header container"><div class="logo-wrapper"><a href="/" class="logo">虞双齐爱折腾</a></div><nav class="site-navbar"><ul id="menu" class="menu"><li class="menu-item"><a class="menu-item-link" href="/">首页</a></li><li class="menu-item"><a class="menu-item-link" href="/post.html">归档</a></li><li class="menu-item"><a class="menu-item-link" href="/tags.html">标签</a></li><li class="menu-item"><a class="menu-item-link" href="/categories.html">分类</a></li><li class="menu-item"><a class="menu-item-link" href="/about.html">关于</a></li></ul></nav></header><div id="mobile-panel"><main id="main" class="main bg-llight"><div class="content-wrapper"><div id="content" class="content container"><article class="post bg-white"><header class="post-header"><h1 class="post-title">TiKV源码解析系列-multi-raft设计与实现</h1><div class="post-meta"><span class="post-time">2016-12-31</span><div class="post-category"><a href="/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E4%B8%8E%E5%BC%80%E5%8F%91.html">编程语言与开发</a></div><span class="more-meta">约 5496 字 </span><span class="more-meta">预计阅读 11 分钟</span></div></header><div class="post-toc" id="post-toc"><h2 class="post-toc-title">文章目录</h2><div class="post-toc-content always-active"><nav id="TableOfContents"><ul><li><ul><li><ul><li><a href="#placement-driver">Placement Driver</a></li><li><a href="#raftstore">Raftstore</a><ul><li><a href="#region">Region</a></li><li><a href="#rocksdb-keys-prefix">RocksDB / Keys Prefix</a></li><li><a href="#peer-storage">Peer Storage</a></li><li><a href="#peer">Peer</a></li><li><a href="#multi-raft">Multi Raft</a></li></ul></li><li><a href="#server">Server</a></li><li><a href="#总结">总结</a></li></ul></li></ul></li></ul></nav></div></div><div class="post-content"><p>本系列文章主要面向 TiKV 社区开发者，重点介绍 TiKV 的系统架构，源码结构，流程解析。目的是使得开发者阅读之后，能对 TiKV 项目有一个初步了解，更好的参与进入 TiKV 的开发中。<br>需要注意，TiKV 使用 Rust 语言编写，用户需要对 Rust 语言有一个大概的了解。另外，本系列文章并不会涉及到 TiKV 中心控制服务 Placement Driver(PD) 的详细介绍，但是会说明一些重要流程 TiKV 是如何与 PD 交互的。<br>TiKV 是一个分布式的 KV 系统，它采用 Raft 协议保证数据的强一致性，同时使用 MVCC + 2PC 的方式实现了分布式事务的支持。<br>本文为本系列文章第二节。</p><h3 id="placement-driver">Placement Driver</h3><p>在继续之前，我们先简单介绍一下 Placement Driver(PD)。PD 是 TiKV 的全局中央控制器，存储整个 TiKV 集群的元数据信息，负责整个 TiKV 集群的调度，全局 ID 的生成，以及全局 TSO 授时等。</p><p>PD 是一个非常重要的中心节点，它通过集成 etcd，自动的支持了分布式扩展以及 failover，解决了单点故障问题。关于 PD 的详细介绍，后续我们会新开一篇文章说明。</p><p>在 TiKV 里面，跟 PD 的交互是放在源码的 pd 目录下，现在跟 PD 的交互都是通过自己定义的 RPC 实现，协议非常简单，在 pd/mod.rs 里面我们直接提供了用于跟 PD 进行交互的 Client trait，以及实现了 RPC Client。</p><p>PD 的 Client trait 非常简单，多数都是对集群元信息的 set/get 操作，需要额外注意的几个：</p><p>bootstrap_cluster：当我们启动一个 TiKV 服务的时候，首先需要通过 is_cluster_bootstrapped 来判断整个 TiKV 集群是否已经初始化，如果还没有初始化，我们就会在该 TiKV 服务上面创建第一个 region。</p><p>region_heartbeat：定期 Region 向 PD 汇报自己的相关信息，供 PD 做后续的调度。譬如，如果一个 Region 给 PD 上报的 peers 的数量小于预设的副本数，那么 PD 就会给这个 Region 添加一个新的副本 Peer。</p><p>store_heartbeat：定期 store 向 PD 汇报自己的相关信息，供 PD 做后续调度。譬如，Store 会告诉 PD 当前的磁盘大小，以及剩余空间，如果 PD 发现空间不够了，就不会考虑将其他的 Peer 迁移到这个 Store 上面。</p><p>ask_split/report_split：当 Region 发现自己需要 split 的时候，就 ask_split 告诉 PD，PD 会生成新分裂 Region 的 ID ，当 Region 分裂成功之后，会 report_split 通知 PD。</p><p>注意，后面我们会让 PD 支持 gRPC 协议，所以 Client API 到时候可能会有变更。</p><h3 id="raftstore">Raftstore</h3><p>因为 TiKV 目标是支持 100 TB+ 以上的数据，一个 Raft 集群是铁定没法支持这么多数据的，所以我们需要使用多个 Raft 集群，也就是 Multi Raft。在 TiKV 里面，Multi Raft 的实现是在 Raftstore 完成的，代码在 raftstore/store 目录。</p><h4 id="region">Region</h4><p>因为我们要支持 Multi Raft，所以我们需要将数据进行分片处理，让每个 Raft 单独负责一部分数据。</p><p>通常的数据分片算法就是 Hash 和 Range，TiKV 使用的 Range 来对数据进行数据分片。为什么使用 Range，主要原因是能更好的将相同前缀的 key 聚合在一起，便于 scan 等操作，这个 Hash 是没法支持的，当然，在 split/merge 上面 Range 也比 Hash 好处理很多，很多时候只会涉及到元信息的修改，都不用大范围的挪动数据。</p><p>当然，Range 有一个问题在于很有可能某一个 Region 会因为频繁的操作成为性能热点，当然也有一些优化的方式，譬如通过 PD 将这些 Region 调度到更好的机器上面，提供 Follower 分担读压力等。</p><p>总之，在 TiKV 里面，我们使用 Range 来对数据进行切分，将其分成一个一个的 Raft Group，每一个 Raft Group，我们使用 Region 来表示。</p><p>Region 的 protobuf 协议定义如下：</p><pre><code>message Region {
    optional uint64 id                  = 1 [(gogoproto.nullable) = false];
    optional bytes  start_key           = 2;
    optional bytes  end_key             = 3;
    optional RegionEpoch region_epoch   = 4;
    repeated Peer   peers               = 5;
}

message RegionEpoch {
    optional uint64 conf_ver    = 1 [(gogoproto.nullable) = false];
    optional uint64 version     = 2 [(gogoproto.nullable) = false];
}

message Peer {      
    optional uint64 id          = 1 [(gogoproto.nullable) = false]; 
    optional uint64 store_id    = 2 [(gogoproto.nullable) = false];
}
</code></pre><p><strong>id</strong>：Region 的唯一表示，通过 PD 全局唯一分配。</p><p><strong>start_key, end_key</strong>：用来表示这个 Region 的范围 [start_key, end_key)，对于最开始的 region，start 和 end key 都是空，TiKV 内部会特殊处理。</p><p><strong>region_epoch</strong>：当一个 Region 添加或者删除 Peer，或者 split 等，我们就会认为这个 Region 的 epoch 发生的变化，RegionEpoch 的 conf_ver 会在每次做 ConfChange 的时候递增，而 version 则是会在每次做 split/merge 的时候递增。</p><p><strong>peers</strong>：当前 Region 包含的节点信息。对于一个 Raft Group，我们通常有三个副本，每个副本我们使用 Peer 来表示，Peer 的 id 也是全局由 PD 分配，而 store_id 则表明这个 Peer 在哪一个 Store 上面。</p><h4 id="rocksdb-keys-prefix">RocksDB / Keys Prefix</h4><p>对于实际数据存储，无论是 Raft Meta，Log，还是 State Machine 的 data，我们都存到一个 RocksDB 实例里面。关于 RocksDB，可以详细参考 <a href="https://github.com/facebook/rocksdb">facebook/rocksdb</a>。</p><p>我们使用不同的前缀来对 Raft 以及 State Machine 等数据进行区分，具体可以参考 raftstore/store/keys.rs，对于 State Machine 实际的 data 数据，我们统一添加 ‘z’ 前缀。而对于其他会存在本地的元数据（包括 Raft），我们统一添加 0x01 前缀。</p><p>这里简单说明一下一些重要元数据的 Key 格式，我们忽略最开始的 0x01 前缀。</p><ul><li><p>0x01：用于存放StoreIdent，在初始化这个 Store 的时候，我们会将 Store 的 Cluster ID，Store ID 等信息存储到这个 key 里面。</p></li><li><p>0x02：用来存储 Raft 一些信息，0x02 之后会紧跟该 Raft Region 的 ID（8字节大端序 ），然后在紧跟一个 Suffix 来标识不同的子类型：</p><ul><li><p>0x01：用于存放 Raft Log，后面紧跟 Log Index（8字节大端序）</p></li><li><p>0x02：用于存放 RaftLocalState</p></li><li><p>0x03：用于存放 RaftApplyState</p></li></ul></li><li><p>0x03：用来存储 Region 本地的一些元信息，0x03 之后紧跟 Raft Region ID，随后在紧跟一个 Suffix 来表示不同的子类型：</p><ul><li>0x01：用于存放 RegionLocalState</li></ul></li></ul><p>对于上面提到的几个类型，都在 protobuf 里面定义：</p><pre><code>message RaftLocalState {
    optional eraftpb.HardState hard_state        = 1;
    optional uint64 last_index                  = 2;
}

message RaftApplyState {
    optional uint64 applied_index               = 1;
    optional RaftTruncatedState truncated_state = 2;
}

enum PeerState {
    Normal       = 0;
    Applying     = 1;
    Tombstone    = 2;
}

message RegionLocalState {
    optional PeerState state        = 1;
    optional metapb.Region region   = 2;
}
</code></pre><p><strong>RaftLocalState</strong>： 用于存放当前 Raft 的 HardState 以及最后一个 Log index。</p><p><strong>RaftApplyState</strong>： 用于存放当前 Raft 最后 apply 的 Log index 以及被 truncated 的 Log 信息。</p><p><strong>RegionLocalStaste</strong>： 用于存放 Region 信息以及在该 Store 上面对应的 Peer 状态，Normal 表明是一个正常的 Peer，Applying 表明该 Peer 还没做完 apply snapshot 的操作，而 Tombstone 则表明该 Peer 已经被移除出了 Region，不能在参与到 Raft Group 里面。</p><h4 id="peer-storage">Peer Storage</h4><p>前面已经知道，我们通过 RawNode 来使用 Raft。因为一个 Region 对应的一个 Raft Group，Region 里面的 Peer 就对应的是一个 Raft 副本。所以，我们在 Peer 里面封装了对 RawNode 的操作。</p><p>要使用 Raft，我们需要定义自己的 Storage，这在 raftstore/store/peer_storage.rs 的 PeerStorage 类里面实现。</p><p>当创建 PeerStorage 的时候，首先我们会从 RocksDB 里面得到该 Peer 之前的 RaftLocalState，RaftApplyState，以及 last_term 等，这些会缓存到内存里面，便于后续的快速度访问。</p><p>PeerStorage 需要注意几个地方：</p><p>首先就是 RAFT_INIT_LOG_TERM 和 RAFT_INIT_LOG_INDEX，它们的值都是 5（只要大于 1 都可以）。在 TiKV 里面，一个 Peer 的创建有如下几种方式：</p><ol><li><p>主动创建，通常对于第一个 Region 的第一个副本 Peer，我们采用这样的创建方式，初始化的时候，我们会将它的 Log Term 和 Index 设置为 5。</p></li><li><p>被动创建，当一个 Region 添加一个副本 Peer 的时候，当这个 ConfChange 命令被 applied 之后， Leader 会给这个新增 Peer 所在的 Store 发送 Message，Store 收到这个 Message 之后，发现并没有相应的 Peer 存在，并且确定这个 Message 是合法的，就会创建一个对应的 Peer，但此时这个 Peer 是一个未初始化的 Peer，不知道所在的 Region 任何的信息，我们使用 0 来初始化它的 Log Term 和 Index。Leader 就能知道这个 Follower 并没有数据（0 到 5 之间存在 Log 缺口），Leader 就会给这个 Follower 直接发送 snapshot。</p></li><li><p>Split 创建，当一个 Region 分裂成两个 Region，其中一个 Region 会继承分裂之前 Region 的元信息，只是会将自己的 Range 范围修改。而另一个 Region 相关的元信息，则会新建，新建的这个 Region 对应的 Peer，初始的 Log Term 和 Index 也是 5，因为这时候 Leader 和 Follower 都有最新的数据，不需要 snapshot。（注意：实际 Split 的情况非常的复杂，有可能也会出现发送 snapshot 的情况，但这里不做过多说明）。</p></li></ol><p>然后就是需要注意 snapshot 的处理。无论 generate 还是 apply snapshot，都是一件比较费时的操作，为了不让 snapshot 的处理卡主整个 Raft 线程，PeerStore 都是会先只同步更新 snapshot 相关的元信息，这样就不用阻碍后续的 Raft 流程，然后会在另一个线程异步的进行 snapshot 的操作。PeerStorage 会维护一个 snapshot 的 state，如下：</p><pre><code>pub enum SnapState {
    Relax,
    Generating(Receiver&lt;Snapshot&gt;),
    Applying(Arc&lt;AtomicUsize&gt;),
    ApplyAborted,
}
</code></pre><p>这里注意 Generating 是一个 channel Receiver，当异步 snapshot 生成好之后，就会给这个 channel 发送消息，这样下一次 Raft 检查的时候，就能直接从这个 channel 得到 snapshot 了。Applying 是一个共享的原子整数，这样就能多线程去判断当前 applying 的状态，包括：</p><pre><code>pub const JOB_STATUS_PENDING: usize = 0;
pub const JOB_STATUS_RUNNING: usize = 1;
pub const JOB_STATUS_CANCELLING: usize = 2;
pub const JOB_STATUS_CANCELLED: usize = 3;
pub const JOB_STATUS_FINISHED: usize = 4;
pub const JOB_STATUS_FAILED: usize = 5;
</code></pre><p>譬如，如果状态是 JOB_STATUS_RUNNING，那么表明当前正在进行 applying snapshot 的操作。现阶段，我们是不允许 FAILED 的，也就是如果 apply snapshot 失败，我们会 panic。</p><h4 id="peer">Peer</h4><p>Peer 封装了 Raft RawNode，我们对 Raft 的 Propose，ready 的处理都是在 Peer 里面完成的。</p><p>首先关注 propose 函数，Peer 的 propose 是外部 Client command 的入口。Peer 会判断这个 command 的类型：</p><ul><li><p>如果是只读操作，并且 Leader 仍然是在 lease 有效期内，Leader 就能直接提供 local read，不需要走 Raft 流程。</p></li><li><p>如果是 Transfer Leader 操作，Peer 首先会判断自己还是不是 Leader，同时判断需要变成新 Leader 的 Follower 是不是有足够新的 Log，如果条件都满足，Peer 就会调用 RawNode 的 transfer_leader 命令。</p></li><li><p>如果是 Change Peer 操作，Peer 就会调用 RawNode propose_conf_change。</p></li><li><p>剩下的，Peer 会直接调用 RawNode 的 propose。</p></li></ul><p>在 propose 之前，Peer 也会将这个 command 对应的 callback 存到 PendingCmd 里面，当对应的 log 被 applied 之后，会通过 command 里面唯一的 uuid 找到对应的 callback 调用，并给 Client 返回相应的结果。</p><p>另一个需要关注的就是 Peer 的 handle_raft_ready 系列函数，在之前 Raft 章节里面介绍过，当一个 RawNode ready 之后，我们需要对 ready 里面的数据做一系列处理，包括将 entries 写入 Storage，发送 messages，apply committed_entries 以及 advance 等。这些全都在 Peer 的 handle_raft_ready 系列函数里面完成。</p><p>对于 committed_entries 的处理，Peer 会解析实际的 command，调用对应的处理流程，执行对应的函数，譬如 exec_admin_cmd 就执行 ConfChange，Split 等 admin 命令，而 exec_write_cmd 则执行通常的对 State Machine 的数据操作命令。为了保证数据的一致性，Peer 在 execute 的时候，都只会将修改的数据保存到 RocksDB 的 WriteBatch 里面，然后在最后原子的写入到 RocksDB，写入成功之后，才修改对应的内存元信息。如果写入失败，我们会直接 panic，保证数据的完整性。</p><p>在 Peer 处理 ready 的时候，我们还会传入一个 Transport 对象，用来让 Peer 发送 message，Transport 的 trait 定义如下：</p><pre><code>pub trait Transport: Send + Clone {
    fn send(&amp;self, msg: RaftMessage) -&gt; Result&lt;()&gt;;
}
</code></pre><p>它就只有一个函数 send，TiKV 实现的 Transport 会将需要 send 的 message 发到 Server 层，由 Server 层发给其他的节点。</p><h4 id="multi-raft">Multi Raft</h4><p>Peer 只是单个 Region 的副本，因为 TiKV 是支持 Multi Raft，所以对于一个 Store 来说，我们需要管理多个 Region 的副本，这些都是在 Store 类里面统一进行管理的。</p><p>Store 会保存所有的 Peers 信息，使用：region_peers: HashMap&lt;u64, Peer&gt;</p><p>region_peers 的 key 就是 Region ID，而 Peer 则是该 Region 在该 Store 上面的副本 Peer。</p><p>Store 使用 <a href="https://github.com/carllerche/mio">mio</a> 驱动整个流程（后续我们会使用 <a href="https://github.com/tokio-rs/tokio-core">tokio-core</a> 来简化异步逻辑处理）。</p><p>我们在 mio 里面注册一个 base Raft Tick，每隔 100ms，调用一次，Store 会遍历所有的 Peer，一次调用对应的 RawNode tick 函数，驱动 Raft。</p><p>Store 通过 mio 的 notify 机制，接受外面 Client 的请求处理，以及其他 Store 发过来的 Raft message。 譬如收到 Msg::RaftCmd 消息之后，Store 就会调用 propose_raft_command 来处理，而收到 Msg::RaftMessage 消息之后，Store 就会调用 on_raft_message 来处理。</p><p>在每次 EventLoop 循环的最后，也就是 mio 的 tick 回调里面，Store 会进行 on_raft_ready 的处理：</p><ol><li><p>Store 会遍历所有的 ready Peers，调用 handle_raft_ready_append，我们会使用一个 WriteBatch 来处理所有的 ready append 数据，同时保存相关的结果。</p></li><li><p>如果 WriteBatch 成功，会依次调用 post_raft_ready_append，这里主要用来处理Follower 的消息发送（Leader 的消息已经在 handle_raft_ready_append 里面完成）。</p></li><li><p>然后，Store 会依次调用 handle_raft_ready_apply，apply 相关 committed entries，然后调用 on_ready_result 处理最后的结果。</p></li></ol><h3 id="server">Server</h3><p>Server 层就是 TiKV 的网络层，现阶段，TiKV 使用 mio 来实现整个网络的处理，而网络协议则是使用自定义的，如下：</p><pre><code>message = header + body 
header:  | 0xdaf4(2 bytes magic value) | 0x01(version 2 bytes) | msg_len(4 bytes) | msg_id(8 bytes) |
</code></pre><p>任何一个 message，我们都使用 header + body 的方式，body 就是实际的 message 数据，使用 protobuf 编码，而 header，首先就是两个字节的 magic value，0xdaf4，然后就是版本号，再就是 message 的整个长度，以及 message 的唯一 ID。</p><p>对于 mio，在 Linux 下面就是封装的 epoll，所以熟悉 epoll 的用户应该能非常方便的使用 mio 进行网络开发，简单流程如下：</p><ul><li><p>bind 一个端口，生成一个 TcpListener 对象，并且 register 到 mio。</p></li><li><p>处理 TcpListener on_readable 的回调，调用 accept 函数得到生成的 socket TcpStream，register 到 mio，我们后续就用这个 TcpStream 跟客户端进行交互。</p></li><li><p>TcpStream 处理 on_readable 或者 on_writable 的回调。</p></li></ul><p>同时，Server 通过 mio 的 notify 来接受外面发过来的消息，譬如 TiKV 实现的 Transport，就是 Peer 在调用 send 的时候，将这个 message 直接通过 channel 发给 Server，然后在 notify 里面处理，找到对应的 Store connection，再发送给远端的 Store 的。</p><p>对于 snapshot 的发送，Server 会单独新开一个连接，直接使用一个线程同步发送，这样代码逻辑就会简单很多，不需要处理过多的异步 IO 逻辑。而对于接收端来说，在收到一个 message 的时候，会首先看这个 message 的类型，如果发现是 snapshot 的，则会进入接受 snapshot 的流程，会将收到的数据直接发给 snapshot 相关的线程，写到对应的 snapshot 文件里面。如果是其他的 message，也会直接 dispatch 到对应的处理逻辑处理，可以参考 Server 的 on_conn_msg 函数。</p><p>因为 Server 就是对网络 IO 的处理，逻辑比较简单，这里就不过多说明，但是，鉴于现阶段 TiKV 使用的是自定义的网络协议，并不利于跟外部其他客户端的对接，并且也没有 pipeline，stream 等优秀特性的 支持，所以后续我们会换成 gRPC。</p><h3 id="总结">总结</h3><p>这里，我们解释了 TiKV 核心的 Raft 库，Multi Raft。在后续的章节，我们会介绍 Transaction，Coprocessor 以及 PD 是如何对整个集群进行变更的。<br>（第二部分完结）</p></div><div style="height:130px"><div class="post-copyright" style="float:left"><p class="copyright-item"><span class="item-title">文章作者</span> <span class="item-content">虞双齐</span></p><p class="copyright-item"><span class="item-title">上次更新</span> <span class="item-content">2016-12-31</span></p><p class="copyright-item"><span class="item-title">许可协议</span> <span class="item-content"><a rel="license noopener" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank">CC BY-NC-ND 4.0</a></span></p></div><div class="post-copyright" style="float:right"><a href="https://info.flagcounter.com/8B1z"><img src="https://s05.flagcounter.com/countxl/8B1z/bg_FFFFFF/txt_000000/border_CCCCCC/columns_4/maxflags_12/viewers_0/labels_0/pageviews_1/flags_0/percent_0/" alt="Flag Counter" border="0"></a></div></div><div class="post-reward"><input type="checkbox" name="reward" id="reward" hidden> <label class="reward-button" for="reward">赞赏支持</label><div class="qr-code"><label class="qr-code-image" for="reward"><img class="image" src="/img/donateMe_wechat.png"> <span>微信打赏</span></label></div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/rust.html">rust</a> <a href="/tags/golang.html">golang</a> <a href="/tags/mysql.html">mysql</a></div><nav class="post-nav"><a class="prev" href="/blog/2016/fen-bu-shi-ji-tong-ce-shi-na-xie-shi-er--xin-xin-de-hui-mie-yu-chong-jian.html"><i class="iconfont icon-left"></i> <span class="prev-text nav-default">分布式系统测试那些事儿-信心的毁灭与重建</span> <span class="prev-text nav-mobile">上一篇</span> </a><a class="next" href="/blog/2016/tikv-yuan-ma-jie-xi-ji-lie--ru-he-shi-yong--raft.html"><span class="next-text nav-default">TiKV源码解析系列-如何使用Raft</span> <span class="prev-text nav-mobile">下一篇</span> <i class="iconfont icon-right"></i></a></nav></footer><div id="comments-gitment"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitment@0.0.3/style/default.min.css" crossorigin="anonymous"><script src="https://cdn.jsdelivr.net/npm/gitment@0.0.3/dist/gitment.browser.min.js" crossorigin="anonymous"></script><script type="text/javascript">const gitment = new Gitment({
    id: '转_TiKV源码解析系列-multi-raft设计与实现.md',
    title: 'TiKV源码解析系列-multi-raft设计与实现',
    link: decodeURI(location.href),
    desc: '本系列文章主要面向 TiKV 社区开发者，重点介绍 TiKV 的系统架构，源码结构，流程解析。目的是使得开发者阅读之后，能对 TiKV 项目有一个初步了解，更好的参与进入',
    owner: 'ysqi',
    repo: 'ysqi.github.io',
    oauth: {
      client_id: '639ba986c281b9fcf382',
      client_secret: 'b5862135f283dd308a7f226c06980cf50b8e82f1'
    }
  })
  gitment.render('comments-gitment')</script><noscript>Please enable JavaScript to view the <a href="https://github.com/imsun/gitment">comments powered by gitment.</a></noscript></article></div></div></main><footer id="footer" class="footer"><div class="social-links"><a href="mailto:ysqi@yushuangqi.com" rel="me" class="iconfont icon-email" title="email"></a> <a href="http://github.com/ysqi" rel="me" class="iconfont icon-github" title="github"></a> <a href="https://weibo.com/234665601" rel="me" class="iconfont icon-weibo" title="weibo"></a> <a href="https://www.zhihu.com/people/_ysqi/" rel="me" class="iconfont icon-zhihu" title="zhihu"></a> <a href="https://yushuangqi.com/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a></div><div class="copyright"><span class="power-by">Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a> </span><span class="division">|</span> <span class="theme-info">Theme - <a class="theme-link" href="https://github.com/xianmin/hugo-theme-jane">Jane</a> </span><span class="copyright-year">&copy; 2014 - 2018 <span class="heart"><i class="iconfont icon-heart"></i> </span><span class="author">虞双齐</span></span></div></footer><div class="back-to-top" id="back-to-top"><i class="iconfont icon-up"></i></div></div><script src="/lib/highlight/highlight.pack.js?v=20171001"></script><script type="text/javascript" src="https://cdn.bootcss.com/jquery/3.2.1/jquery.min.js" crossorigin="anonymous"></script><script type="text/javascript" src="https://cdn.bootcss.com/slideout/1.0.1/slideout.min.js" crossorigin="anonymous"></script><script type="text/javascript" src="https://cdn.bootcss.com/fancybox/3.1.20/jquery.fancybox.min.js" crossorigin="anonymous"></script><script type="text/javascript" src="/dist/jane.min.js?v=2.7.0"></script><script type="text/javascript">window.MathJax = {
      showProcessingMessages: false,
      messageStyle: 'none'
    };</script><script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script><script id="baidu_analytics">var _hmt = _hmt || [];
  (function() {
    if (window.location.hostname === 'localhost') return;
    var hm = document.createElement("script"); hm.async = true;
    hm.src = "https://hm.baidu.com/hm.js?a16b3275b071ec0efc507a05422a7156";
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(hm, s);
  })();</script></body></html>